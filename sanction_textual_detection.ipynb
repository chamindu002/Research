{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwdQ9evARvXTREuC/lcyqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chamindu002/Research/blob/main/sanction_textual_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4LZaDtSjy4N4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70de02bf-07ff-48e5-92c3-1df77ee1c14e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supabase in /usr/local/lib/python3.12/dist-packages (2.27.1)\n",
            "Requirement already satisfied: realtime==2.27.1 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.27.1)\n",
            "Requirement already satisfied: supabase-functions==2.27.1 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.27.1)\n",
            "Requirement already satisfied: storage3==2.27.1 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.27.1)\n",
            "Requirement already satisfied: supabase-auth==2.27.1 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.27.1)\n",
            "Requirement already satisfied: postgrest==2.27.1 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.27.1)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.12/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: yarl>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from supabase) (1.22.0)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from postgrest==2.27.1->supabase) (2.1.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.12/dist-packages (from postgrest==2.27.1->supabase) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from realtime==2.27.1->supabase) (4.15.0)\n",
            "Requirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.12/dist-packages (from realtime==2.27.1->supabase) (15.0.1)\n",
            "Requirement already satisfied: pyiceberg>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from storage3==2.27.1->supabase) (0.10.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.27.1->supabase) (2.10.1)\n",
            "Requirement already satisfied: strenum>=0.4.15 in /usr/local/lib/python3.12/dist-packages (from supabase-functions==2.27.1->supabase) (0.4.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.22.0->supabase) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.22.0->supabase) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.1.0->postgrest==2.27.1->supabase) (25.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest==2.27.1->supabase) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest==2.27.1->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest==2.27.1->supabase) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest==2.27.1->supabase) (0.4.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=5.5 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (6.2.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (8.3.1)\n",
            "Requirement already satisfied: fsspec>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (2025.3.0)\n",
            "Requirement already satisfied: mmh3<6.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (5.2.0)\n",
            "Requirement already satisfied: pyparsing<4.0.0,>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (3.2.5)\n",
            "Requirement already satisfied: pyroaring<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (2.32.4)\n",
            "Requirement already satisfied: rich<15.0.0,>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (13.9.4)\n",
            "Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (2.4.0)\n",
            "Requirement already satisfied: strictyaml<2.0.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (1.7.3)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from pyiceberg>=0.10.0->storage3==2.27.1->supabase) (9.1.2)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.27.1->supabase) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.27.1->supabase) (2.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.27.1->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.27.1->supabase) (4.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.20.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.20.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=10.11.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=10.11.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (2.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from strictyaml<2.0.0,>=1.7.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.27.1->supabase) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=10.11.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.6.0->strictyaml<2.0.0,>=1.7.0->pyiceberg>=0.10.0->storage3==2.27.1->supabase) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode -q\n",
        "!pip install torch pandas openpyxl -q\n",
        "!pip install supabase\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Drive mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mSyUidHMnW4",
        "outputId": "66f35a42-133e-4085-b086-b8d2ef2a4564"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Model Classes"
      ],
      "metadata": {
        "id": "-rE5S6P9MuLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=96, hidden=192):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden*2, 192)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # last timestep\n",
        "        out = torch.tanh(self.fc(out))\n",
        "        return out\n",
        "\n",
        "class Siamese(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = NameEncoder(vocab_size)\n",
        "        self.cosine = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        v1 = self.encoder(x1)\n",
        "        v2 = self.encoder(x2)\n",
        "        return self.cosine(v1, v2)"
      ],
      "metadata": {
        "id": "6z593m6bMrgn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Model**"
      ],
      "metadata": {
        "id": "UdfBC8wdM2Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "load_path = \"/content/drive/My Drive/Research/Data/siamese_name_matcher_best_intials.pt\"\n",
        "\n",
        "print(f\"Loading model from: {load_path}\")\n",
        "checkpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "# Restore config\n",
        "char2idx = checkpoint['char2idx']\n",
        "max_len = checkpoint['max_len']\n",
        "vocab_size = checkpoint['vocab_size']\n",
        "\n",
        "model = Siamese(vocab_size).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(\"✅ Model loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_i8vBq6MyZt",
        "outputId": "042caab8-beaf-4eee-ee75-9870922d03d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/My Drive/Research/Data/siamese_name_matcher_best_intials.pt\n",
            "✅ Model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Helper Functions"
      ],
      "metadata": {
        "id": "aGb211hWM81E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_transliterate(s):\n",
        "    if s is None or pd.isna(s): return \"\"\n",
        "    s = unidecode(str(s))\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"[^a-z\\s\\.]\", \" \", s)  # Keep dots for initials\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def encode_name(name):\n",
        "    name = normalize_and_transliterate(name)\n",
        "    seq = [char2idx.get(c, char2idx[\"<UNK>\"]) for c in name[:max_len]]\n",
        "    seq += [char2idx[\"<PAD>\"]] * (max_len - len(seq))\n",
        "    return torch.tensor([seq], dtype=torch.long).to(device)\n",
        "\n",
        "def get_similarity(name1, name2):\n",
        "    if not name1 or not name2: return 0.0\n",
        "    t1 = encode_name(name1)\n",
        "    t2 = encode_name(name2)\n",
        "    with torch.no_grad():\n",
        "        score = model(t1, t2).item()\n",
        "    return score\n",
        "\n",
        "def extract_birth_year(birth_date):\n",
        "    \"\"\"Extract set of possible birth years from various string formats.\"\"\"\n",
        "    if pd.isna(birth_date) or not str(birth_date).strip():\n",
        "        return set()\n",
        "    bd = str(birth_date).strip()\n",
        "    years = set()\n",
        "\n",
        "    # 1. Format: YYYY-MM-DD (e.g., 2002-07-31)\n",
        "    if re.match(r'^\\d{4}-\\d{2}-\\d{2}', bd):\n",
        "        years.add(int(bd[:4]))\n",
        "\n",
        "    # 2. Format: DD/MM/YYYY or DD-MM-YYYY (e.g., 31/07/2002)\n",
        "    # This finds any 4 digits at the end of a date string\n",
        "    elif re.search(r'(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4})', bd):\n",
        "        match = re.search(r'(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4})', bd)\n",
        "        years.add(int(match.group(3)))\n",
        "\n",
        "    # 3. Format: Year only (e.g., 1973)\n",
        "    elif re.match(r'^\\d{4}$', bd):\n",
        "        years.add(int(bd))\n",
        "\n",
        "    # 4. Range: 1970-1980\n",
        "    elif '-' in bd and len(bd) > 5:\n",
        "        parts = bd.split('-')\n",
        "        # Check if parts look like years\n",
        "        if len(parts) == 2 and parts[0].strip().isdigit() and parts[1].strip().isdigit():\n",
        "            start, end = int(parts[0]), int(parts[1])\n",
        "            years.update(range(start, end+1))\n",
        "\n",
        "    # 5. Multiple years: 1970, 1972\n",
        "    elif ',' in bd:\n",
        "        for y in bd.split(','):\n",
        "            y = y.strip()\n",
        "            if y.isdigit() and len(y)==4:\n",
        "                years.add(int(y))\n",
        "\n",
        "    return years\n",
        "\n",
        "def fields_match(cust_val, src_val, field):\n",
        "    \"\"\"Check if fields match (exact, ignoring case).\"\"\"\n",
        "    if pd.isna(cust_val) or not str(cust_val).strip():\n",
        "        return True\n",
        "    cust = str(cust_val).strip().upper()\n",
        "    src = str(src_val).strip().upper()\n",
        "    return cust == src"
      ],
      "metadata": {
        "id": "0zKDqAzUNGZ4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Master Sanction/PEP List**"
      ],
      "metadata": {
        "id": "cEXIgmGMNJM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_path = \"/content/drive/My Drive/Research/Data/testing_data.csv\"\n",
        "master_df = pd.read_csv(master_path)\n",
        "master_df.fillna(\"\", inplace=True)\n",
        "master_df['NAME'] = master_df['NAME'].astype(str).str.strip().str.upper()\n",
        "master_df['ALIAS'] = master_df['ALIAS'].astype(str).str.strip()\n",
        "master_df['BIRTH_DATE'] = master_df['BIRTH_DATE'].astype(str).str.strip()\n",
        "master_df['ID'] = master_df['ID'].astype(str).str.strip()\n",
        "master_df['NATIONALITY'] = master_df['NATIONALITY'].astype(str).str.strip()\n",
        "\n",
        "print(f\"Loaded master list: {len(master_df)} records\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWQ-zuBKNIE5",
        "outputId": "71ff0885-5d87-4214-db85-ecbfecff935d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded master list: 32 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Customer List (Update Path After Uploading File)**"
      ],
      "metadata": {
        "id": "2sals6Z-NSS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from supabase import create_client\n",
        "import pandas as pd\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONNECT TO SUPABASE\n",
        "# ==========================================\n",
        "# (Note: In production, consider using environment variables for keys)\n",
        "SUPABASE_URL = \"https://gntqcwkcsxevjqiabmmz.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdudHFjd2tjc3hldmpxaWFibW16Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NzcyNTgwNCwiZXhwIjoyMDgzMzAxODA0fQ.6YsILjaAVmWiJkQ3l_cauLxX0yM9LymEvwjBKN1c-ac\"\n",
        "\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "print(\"Fetching data from Supabase...\")\n",
        "try:\n",
        "    response = supabase.table(\"submissions\").select(\"*\").execute()\n",
        "    customer_df = pd.DataFrame(response.data)\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching data: {e}\")\n",
        "    customer_df = pd.DataFrame() # Empty fallback\n",
        "\n",
        "# ==========================================\n",
        "# 2. RENAME & CLEAN DATA\n",
        "# ==========================================\n",
        "if not customer_df.empty:\n",
        "    # Rename columns to match the logic used in the rest of your notebook\n",
        "    # Mapping: DB Column -> Code Column\n",
        "    rename_map = {\n",
        "        \"full_name\": \"NAME\",\n",
        "        \"nic\": \"ID\",\n",
        "        \"nationality\": \"NATIONALITY\",\n",
        "        \"alias\": \"ALIAS\",\n",
        "        \"dob\": \"BIRTH_DATE\",\n",
        "        \"created_at\": \"CREATED_AT\"\n",
        "    }\n",
        "    customer_df.rename(columns=rename_map, inplace=True)\n",
        "\n",
        "    # Fill NaNs\n",
        "    customer_df.fillna(\"\", inplace=True)\n",
        "\n",
        "    # Standardize Text Columns\n",
        "    customer_df['NAME'] = customer_df['NAME'].astype(str).str.strip().str.upper()\n",
        "\n",
        "    # Handle optional columns if they exist, else create empty\n",
        "    for col in ['ALIAS', 'BIRTH_DATE', 'NATIONALITY', 'CREATED_AT']:\n",
        "        if col in customer_df.columns:\n",
        "            customer_df[col] = customer_df[col].astype(str).str.strip()\n",
        "        else:\n",
        "            customer_df[col] = \"\"\n",
        "\n",
        "    # Specific Cleaning for ID to handle Scientific Notation (2.00221E+11 -> 200221...)\n",
        "    def clean_id_string(val):\n",
        "        if not val: return \"\"\n",
        "        s = str(val).strip()\n",
        "        try:\n",
        "            # If it looks like a float or scientific notation, convert to float then int then str\n",
        "            if \"E\" in s.upper() or \".\" in s:\n",
        "                return str(int(float(s)))\n",
        "        except:\n",
        "            pass\n",
        "        return s\n",
        "\n",
        "    if 'ID' in customer_df.columns:\n",
        "        customer_df['ID'] = customer_df['ID'].apply(clean_id_string)\n",
        "    else:\n",
        "        customer_df['ID'] = \"\"\n",
        "\n",
        "print(f\"Loaded customer list: {len(customer_df)} records\")\n",
        "print(customer_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LasQ5jugNPgS",
        "outputId": "e879330c-d85a-4ad5-9446-53ad92c74814"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from Supabase...\n",
            "Loaded customer list: 12 records\n",
            "                                     id                     NAME  \\\n",
            "0  0636e8eb-fe90-4a16-a19f-d1988f28c221         D G C D RASHMIKA   \n",
            "1  de2ee86a-19e8-4619-b315-4268ca7eaf29        CHARUKA BANDARA D   \n",
            "2  c9be7a68-304e-4422-bff1-2ab6eff8de7c         A H B K SAMANTHA   \n",
            "3  c49e53d2-4881-46e9-852c-860b373a6800    W. M. CHATHURA DESHAN   \n",
            "4  65578cb0-252b-408d-8957-ba3722ad7875  K. G. N. PRIYADARSHANEE   \n",
            "\n",
            "             ID NATIONALITY    ALIAS  BIRTH_DATE email  \\\n",
            "0  200221302925    srilanka  Chamiya  2002-07-31         \n",
            "1                  srilanka    Charu  2002-10-18         \n",
            "2                  srilanka     sama                     \n",
            "3                  srilanka           1994-04-12         \n",
            "4    886543102V    srilanka   Nayana                     \n",
            "\n",
            "                                        note image_path image_url  \\\n",
            "0  Category: sanction, Dataset: testing_data                        \n",
            "1  Category: sanction, Dataset: testing_data                        \n",
            "2       Category: pep, Dataset: testing_data                        \n",
            "3  Category: sanction, Dataset: testing_data                        \n",
            "4       Category: pep, Dataset: testing_data                        \n",
            "\n",
            "                         CREATED_AT  \n",
            "0  2026-01-07T02:47:09.374171+00:00  \n",
            "1  2026-01-07T02:47:09.374171+00:00  \n",
            "2  2026-01-07T02:47:09.374171+00:00  \n",
            "3  2026-01-07T02:47:09.374171+00:00  \n",
            "4  2026-01-07T02:47:09.374171+00:00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Matching**"
      ],
      "metadata": {
        "id": "ZHTnkqpcNZR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "SIMILARITY_THRESHOLD = 0.50\n",
        "RISK_THRESHOLD = 0.50\n",
        "\n",
        "WEIGHTS = {\n",
        "    \"name\": 0.6,\n",
        "    \"dob\": 0.2,\n",
        "    \"nationality\": 0.1,\n",
        "    \"id\": 0.1\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# 2. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "def safe_get(row, keys, default=\"\"):\n",
        "    for k in keys:\n",
        "        if k in row:\n",
        "            val = row[k]\n",
        "            if pd.notna(val) and str(val).strip():\n",
        "                return str(val).strip()\n",
        "    return default\n",
        "\n",
        "def safe_field_match(cust_val, src_val):\n",
        "    if not cust_val or not src_val:\n",
        "        return 0.0\n",
        "    # Clean up scientific notation (2E+11 -> 200...)\n",
        "    c_str = str(cust_val).strip().upper().replace(\".0\", \"\")\n",
        "    s_str = str(src_val).strip().upper().replace(\".0\", \"\")\n",
        "    if c_str == s_str:\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def birth_year_score(cust_years, src_years):\n",
        "    if not cust_years or not src_years:\n",
        "        return 0.0\n",
        "    if cust_years.intersection(src_years):\n",
        "        return 1.0\n",
        "    return 0.0\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN MATCHING LOOP\n",
        "# ==========================================\n",
        "reports = []\n",
        "print(f\"Starting matching process for {len(customer_df)} customers against {len(master_df)} source records...\")\n",
        "\n",
        "for _, cust in customer_df.iterrows():\n",
        "    # Customer Data\n",
        "    cust_name = cust.get(\"NAME\", \"\")\n",
        "    cust_alias = cust.get(\"ALIAS\", \"\")\n",
        "    cust_birth_years = extract_birth_year(cust.get(\"BIRTH_DATE\", \"\"))\n",
        "    cust_id = cust.get(\"ID\", \"\")\n",
        "    cust_nat = cust.get(\"NATIONALITY\", \"\")\n",
        "\n",
        "    # --- NEW: Get Created At date ---\n",
        "    cust_created_at = cust.get(\"CREATED_AT\", \"\")\n",
        "\n",
        "    best_match = None\n",
        "    best_risk = -1.0\n",
        "    best_details = {}\n",
        "\n",
        "    for _, src in master_df.iterrows():\n",
        "        src_id = src.get(\"ID\", \"\")\n",
        "\n",
        "        # --- SCORE CALCULATION ---\n",
        "\n",
        "        # 1. NAME & ALIAS CHECK\n",
        "        sims = {\"NAME\": get_similarity(cust_name, src.get(\"NAME\", \"\"))}\n",
        "\n",
        "        # Check Source Alias\n",
        "        if \"ALIAS\" in src and pd.notna(src[\"ALIAS\"]):\n",
        "            sims[\"ALIAS\"] = get_similarity(cust_name, src[\"ALIAS\"])\n",
        "\n",
        "        # Check Customer Alias vs Source Name\n",
        "        if cust_alias:\n",
        "            sims[\"CUST_ALIAS\"] = get_similarity(cust_alias, src.get(\"NAME\", \"\"))\n",
        "\n",
        "        # Determine best text match\n",
        "        name_match_type, name_score = max(sims.items(), key=lambda x: x[1])\n",
        "\n",
        "        # 2. ATTRIBUTE CHECKS\n",
        "        src_birth_years = extract_birth_year(src.get(\"BIRTH_DATE\", \"\"))\n",
        "        dob_score = birth_year_score(cust_birth_years, src_birth_years)\n",
        "        nat_score = safe_field_match(cust_nat, src.get(\"NATIONALITY\", \"\"))\n",
        "        id_score = safe_field_match(cust_id, src_id)\n",
        "\n",
        "        # 3. DETERMINE FINAL MATCH REASON\n",
        "        final_reason = name_match_type  # e.g. \"NAME\", \"ALIAS\", \"CUST_ALIAS\"\n",
        "\n",
        "        if id_score == 1.0:\n",
        "            final_reason = \"ID_MATCH\"\n",
        "        elif dob_score == 1.0 and name_score >= SIMILARITY_THRESHOLD:\n",
        "            # If it matched on alias + DOB, show \"ALIAS_AND_DOB\"\n",
        "            final_reason = f\"{name_match_type}_AND_DOB\"\n",
        "\n",
        "        # 4. CALCULATE RISK\n",
        "        current_risk_score = (\n",
        "            name_score * WEIGHTS[\"name\"] +\n",
        "            dob_score * WEIGHTS[\"dob\"] +\n",
        "            nat_score * WEIGHTS[\"nationality\"] +\n",
        "            id_score * WEIGHTS[\"id\"]\n",
        "        )\n",
        "\n",
        "        if id_score == 1.0:\n",
        "            current_risk_score = 1.0\n",
        "            confidence = \"CERTAIN\"\n",
        "        else:\n",
        "            match_count = sum([1 for s in [dob_score, nat_score, id_score] if s == 1.0])\n",
        "            if match_count >= 2 and name_score > 0.70:\n",
        "                confidence = \"VERY_HIGH\"\n",
        "            elif match_count >= 1 and name_score > 0.75:\n",
        "                confidence = \"HIGH\"\n",
        "            elif name_score > 0.80:\n",
        "                confidence = \"MEDIUM\"\n",
        "            else:\n",
        "                confidence = \"LOW\"\n",
        "\n",
        "        # 5. SAVE BEST MATCH\n",
        "        if current_risk_score > best_risk:\n",
        "            best_risk = current_risk_score\n",
        "            best_match = src\n",
        "            best_details = {\n",
        "                \"risk_score\": round(current_risk_score, 4),\n",
        "                \"confidence\": confidence,\n",
        "                \"match_reason\": final_reason,\n",
        "                \"score_name\": name_score,\n",
        "                \"score_dob\": dob_score,\n",
        "                \"score_nat\": nat_score,\n",
        "                \"score_id\": id_score\n",
        "            }\n",
        "\n",
        "    # 3c. SAVE TO REPORT\n",
        "    if best_match is not None and (best_details[\"risk_score\"] >= RISK_THRESHOLD or best_details[\"score_id\"] == 1.0):\n",
        "\n",
        "        src_cat = safe_get(best_match, [\"CATEGORY\", \"Category\", \"TYPE\", \"Type\"], \"Unknown\").upper()\n",
        "        src_ds = safe_get(best_match, [\"DATASET\", \"Dataset\", \"SOURCE\", \"Source\"], \"Unknown\").upper()\n",
        "\n",
        "        reports.append({\n",
        "            \"customer_name\": cust_name,\n",
        "            \"created_at\": cust_created_at,  # --- ADDED FIELD ---\n",
        "            \"source_name\": best_match.get(\"NAME\", \"\"),\n",
        "            \"source_alias\": best_match.get(\"ALIAS\", \"\"),\n",
        "            \"TYPE\": src_cat,\n",
        "            \"SOURCE_LIST\": src_ds,\n",
        "\n",
        "            \"status\": \"HIT\" if best_details[\"risk_score\"] >= 0.7 else \"REVIEW\",\n",
        "            \"risk_score\": best_details[\"risk_score\"],\n",
        "            \"confidence\": best_details[\"confidence\"],\n",
        "            \"match_reason\": best_details[\"match_reason\"],\n",
        "\n",
        "            \"SCORE_NAME\": f\"{best_details['score_name']*100:.1f}%\",\n",
        "            \"SCORE_DOB\": f\"{best_details['score_dob']*100:.1f}%\",\n",
        "            \"SCORE_NAT\": f\"{best_details['score_nat']*100:.1f}%\",\n",
        "            \"SCORE_ID\": f\"{best_details['score_id']*100:.1f}%\",\n",
        "\n",
        "            \"customer_id\": cust_id,\n",
        "            \"source_id\": best_match.get(\"ID\", \"\"),\n",
        "            \"customer_dob\": cust.get(\"BIRTH_DATE\", \"\"),\n",
        "            \"source_dob\": best_match.get(\"BIRTH_DATE\", \"\")\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2qneM0NZGvw",
        "outputId": "3e5440cc-754c-4b16-eec0-fde5e6f52e2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting matching process for 12 customers against 32 source records...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate and Save Report**"
      ],
      "metadata": {
        "id": "Hron3sPqNguJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================\n",
        "# 4. SUMMARY REPORTING\n",
        "# ==========================================\n",
        "if reports:\n",
        "    report_df = pd.DataFrame(reports)\n",
        "\n",
        "    # Define clean column order (Added 'created_at')\n",
        "    cols_order = [\n",
        "        \"customer_name\", \"created_at\", \"source_name\", \"TYPE\", \"SOURCE_LIST\",\n",
        "        \"status\", \"risk_score\", \"confidence\", \"match_reason\",\n",
        "        \"SCORE_NAME\", \"SCORE_DOB\", \"SCORE_NAT\", \"SCORE_ID\"\n",
        "    ]\n",
        "    # Filter columns\n",
        "    final_cols = [c for c in cols_order if c in report_df.columns] + \\\n",
        "                 [c for c in report_df.columns if c not in cols_order]\n",
        "    report_df = report_df[final_cols]\n",
        "\n",
        "    # --- PRINT SUMMARY ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MATCHING SUMMARY STATISTICS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total Hits Found: {len(report_df)}\")\n",
        "\n",
        "    print(\"\\n[1] Breakdown by Match Reason:\")\n",
        "    print(report_df['match_reason'].value_counts())\n",
        "\n",
        "    print(\"\\n[2] Breakdown by Confidence:\")\n",
        "    print(report_df['confidence'].value_counts())\n",
        "\n",
        "    print(\"\\n[3] Breakdown by Category (Sanction/PEP):\")\n",
        "    print(report_df['TYPE'].value_counts())\n",
        "\n",
        "    # Save\n",
        "    save_path = '/content/drive/My Drive/Research/Data/customer/sanction_check_report.csv'\n",
        "    report_df.to_csv(save_path, index=False)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Full report saved to: {save_path}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Show preview\n",
        "    print(\"\\nPreview of Top Matches:\")\n",
        "    print(report_df.head(5))\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo matches found above the threshold.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0RdVMU7rPlD",
        "outputId": "f19fd1fa-47d4-41c5-f401-a3b713516b3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MATCHING SUMMARY STATISTICS\n",
            "==================================================\n",
            "Total Hits Found: 12\n",
            "\n",
            "[1] Breakdown by Match Reason:\n",
            "match_reason\n",
            "NAME_AND_DOB    6\n",
            "NAME            4\n",
            "ID_MATCH        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[2] Breakdown by Confidence:\n",
            "confidence\n",
            "VERY_HIGH    5\n",
            "HIGH         3\n",
            "CERTAIN      2\n",
            "LOW          2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[3] Breakdown by Category (Sanction/PEP):\n",
            "TYPE\n",
            "SANCTION    8\n",
            "PEP         4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "Full report saved to: /content/drive/My Drive/Research/Data/customer/sanction_check_report.csv\n",
            "==================================================\n",
            "\n",
            "Preview of Top Matches:\n",
            "             customer_name                        created_at                                source_name      TYPE   SOURCE_LIST status  risk_score confidence  match_reason SCORE_NAME SCORE_DOB SCORE_NAT SCORE_ID source_alias   customer_id     source_id customer_dob  source_dob\n",
            "0         D G C D RASHMIKA  2026-01-07T02:47:09.374171+00:00  DEEGODA GAMAGEI CHAMINDU DENUWAN RASHMIKA  SANCTION  TESTING_DATA    HIT       1.000    CERTAIN      ID_MATCH      87.1%    100.0%    100.0%   100.0%      Chamiya  200221302925  200221302925   2002-07-31  31/07/2002\n",
            "1        CHARUKA BANDARA D  2026-01-07T02:47:09.374171+00:00                   CHARUKA BANDARA DAHANAKA  SANCTION  TESTING_DATA    HIT       0.883  VERY_HIGH  NAME_AND_DOB      97.2%    100.0%    100.0%     0.0%        Charu                               2002-10-18  18/10/2002\n",
            "2         A H B K SAMANTHA  2026-01-07T02:47:09.374171+00:00                           A H B K SAMANTHA       PEP  TESTING_DATA    HIT       0.700       HIGH          NAME     100.0%      0.0%    100.0%     0.0%         sama                                           25/10/1994\n",
            "3    W. M. CHATHURA DESHAN  2026-01-07T02:47:09.374171+00:00                      W. M. CHATHURA DESHAN  SANCTION  TESTING_DATA    HIT       0.900  VERY_HIGH  NAME_AND_DOB     100.0%    100.0%    100.0%     0.0%                                            1994-04-12  12/04/1994\n",
            "4  K. G. N. PRIYADARSHANEE  2026-01-07T02:47:09.374171+00:00                     K. G. N. PRIYADARSHANI       PEP  TESTING_DATA    HIT       1.000    CERTAIN      ID_MATCH      99.1%      0.0%    100.0%   100.0%       Nayana    886543102V    886543102V                         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if reports:\n",
        "#     report_df = pd.DataFrame(reports)\n",
        "#     report_df.to_csv('/content/drive/My Drive/Research/Data/customer/sanction_check_report.csv', index=False)\n",
        "#     print(\"\\n--- SANCTION/PEP MATCH SUMMARY ---\")\n",
        "#     print(report_df)\n",
        "#     print(\"\\nReport saved to: /content/sanction_check_report.csv\")\n",
        "# else:\n",
        "#     print(\"\\nNo matches found for any customers.\")"
      ],
      "metadata": {
        "id": "GQATPTaRNh3q"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}